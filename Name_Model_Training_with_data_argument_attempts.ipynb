{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Name Model Training - with data argument attempts.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEt-8FQSJg6T",
        "outputId": "7418b3fe-5bd0-498a-ff7b-e7054be5916c"
      },
      "source": [
        "!pip install -q -U tensorflow-text==2.4.1\n",
        "!pip install -q -U tf-models-official==2.4.0\n",
        "!pip install -U tfds-nightly\n",
        "!pip install -q -U tensorflow==2.4.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tfds-nightly in /usr/local/lib/python3.7/dist-packages (4.3.0.dev202106250106)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (21.2.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.1.4)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->tfds-nightly) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oP4-y9wKm_J"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSBhACRTK2ay",
        "outputId": "132bc1f2-d52b-489e-9532-51d8c8333249"
      },
      "source": [
        "import os\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "  print('Using TPU')\n",
        "elif tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')\n",
        "else:\n",
        "  raise ValueError('Running on CPU is not recommended.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd2AHbvkLMxM"
      },
      "source": [
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Yrcp5mP6Lgzt",
        "outputId": "4b4d1d9b-ebda-408c-ad88-96e43c060246"
      },
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-256_A-4'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_uncased_L-24_H-1024_A-16\", \"bert_en_wwm_uncased_L-24_H-1024_A-16\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_en_cased_L-24_H-1024_A-16\", \"bert_en_wwm_cased_L-24_H-1024_A-16\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"albert_en_large\", \"albert_en_xlarge\", \"albert_en_xxlarge\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\", \"talking-heads_large\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_large/2',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xlarge/2',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xxlarge/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print('BERT model selected           :', tfhub_handle_encoder)\n",
        "print('Preprocessing model auto-selected:', tfhub_handle_preprocess)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1\n",
            "Preprocessing model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVjfKHqFMU-i"
      },
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=256):\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: a list with the names of string-valued features.\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\n",
        "  # Simple cases (like this example) can skip this step and let\n",
        "  # the next step apply a default truncation to approximately equal lengths.\n",
        "  truncated_segments = segments\n",
        "\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL11ug6rQftc"
      },
      "source": [
        "def build_classifier_model(num_classes = 2):\n",
        "  inputs = dict(\n",
        "      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_word_ids'),\n",
        "      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_mask'),\n",
        "      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_type_ids'),\n",
        "  )\n",
        "\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\n",
        "  net = encoder(inputs)['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(rate=0.1)(net)\n",
        "  net = tf.keras.layers.Dense(num_classes, activation='softmax', name='classifier')(net)\n",
        "  return tf.keras.Model(inputs, net, name='prediction')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDRNkb_lXXoH",
        "outputId": "17a6af2f-b3c9-4abc-9a34-485e1617ccc1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K_IJJEfXLJR"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/name_matching_attempt/combined_negative_samples_argument_v1.pickle\", \"rb\") as handle:\n",
        "  combined_negative_samples_argument_v1 = pickle.load(handle)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67lxLREzXmhV"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/name_matching_attempt/combined_positive_samples_argument_v1_with_abbr.pickle\", \"rb\") as handle:\n",
        "  combined_positive_samples_argument_v1_with_abbr = pickle.load(handle)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6mS6DF4ARbq",
        "outputId": "9698bfbe-6334-4da3-acc6-390719698b08"
      },
      "source": [
        "len(combined_negative_samples_argument_v1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "370140"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olXLFVJwAPml",
        "outputId": "2e8c016d-9c1d-45e3-cc6c-3bf989f0cb65"
      },
      "source": [
        "len(combined_positive_samples_argument_v1_with_abbr)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198053"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO5Hu3XBXuJy"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 10000)\n",
        "pd.set_option('max_colwidth', 10000)\n",
        "pd.set_option(\"max_rows\", 50000)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiqmQVFQX1Lo"
      },
      "source": [
        "list_positive_left = combined_positive_samples_argument_v1_with_abbr['name_left'].tolist()\n",
        "list_positive_right = combined_positive_samples_argument_v1_with_abbr['name_right'].tolist()\n",
        "list_negative_left = combined_negative_samples_argument_v1['name_left'].tolist()\n",
        "list_negative_right = combined_negative_samples_argument_v1['name_right'].tolist()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aZrFug3YmLZ"
      },
      "source": [
        "list_positive_pairs = [[left, right] for left, right in zip(list_positive_left, list_positive_right)]\n",
        "list_negative_pairs = [[left, right] for left, right in zip(list_negative_left, list_negative_right)]\n",
        "\n",
        "n_training_positive = int(len(list_positive_pairs) * 0.8)\n",
        "n_training_negative = int(len(list_negative_pairs) * 0.8)\n",
        "\n",
        "labels_training = [1 for _ in range(n_training_positive)] + [0 for _ in range(n_training_negative)]\n",
        "samples_training = list_positive_pairs[: n_training_positive] + list_negative_pairs[:n_training_negative]\n",
        "\n",
        "labels_validation = [1 for _ in range(len(list_positive_pairs[n_training_positive + 1:]))] + [0 for _ in range(len(list_negative_pairs[n_training_negative + 1:]))]\n",
        "samples_validation = list_positive_pairs[n_training_positive + 1:] + list_negative_pairs[n_training_negative+1:]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a6ql_1rZCjX",
        "outputId": "a96018aa-af12-4ea5-df69-507685646ab1"
      },
      "source": [
        "len(samples_training)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "454554"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_iW-xTLbhdZ",
        "outputId": "c627318d-5ab7-4311-cec1-91d3edb04a0f"
      },
      "source": [
        "len(samples_validation)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "113637"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLm8ocFHbqEw"
      },
      "source": [
        "ds_training_dict = {\n",
        "    'left_name': [sample[0] for sample in samples_training], \n",
        "    'right_name': [sample[1] for sample in samples_training],\n",
        "    'label': labels_training,\n",
        "    }\n",
        "ds_validation_dict = {\n",
        "    'left_name': [sample[0] for sample in samples_validation], \n",
        "    'right_name': [sample[1] for sample in samples_validation],\n",
        "    'label': labels_validation,\n",
        "    }"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QYcXsx7TcEY"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_dataset(ds, is_training, batch_size, bert_preprocess_model):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
        "  num_examples = len(ds['left_name'])\n",
        "  \n",
        "  dataset = dataset.shuffle(num_examples)\n",
        "  if is_training:\n",
        "    dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
        "  dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  return dataset, num_examples"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATOrHhyRmyO"
      },
      "source": [
        "def get_configuration():\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "  metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        'accuracy', dtype=tf.float32)\n",
        "\n",
        "  return metrics, loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cbq1vgHSTYB",
        "outputId": "80261bb1-f4c1-4e61-f6b9-ee2c0b5bf0a9"
      },
      "source": [
        "epochs = 2\n",
        "batch_size = 512\n",
        "init_lr = 1e-5\n",
        "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
        "bert_preprocess_model = make_bert_preprocess_model(['left_name', 'right_name'])\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "  # metric have to be created inside the strategy scope\n",
        "  metrics, loss = get_configuration()\n",
        "\n",
        "  train_dataset, train_data_size = load_dataset(ds_training_dict, True, batch_size, bert_preprocess_model)\n",
        "  steps_per_epoch = train_data_size // batch_size\n",
        "  num_train_steps = steps_per_epoch * epochs\n",
        "  num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "  validation_dataset, validation_data_size = load_dataset(ds_validation_dict, False, batch_size, bert_preprocess_model)\n",
        "  validation_steps = validation_data_size // batch_size\n",
        "\n",
        "  classifier_model = build_classifier_model()\n",
        "\n",
        "  optimizer = optimization.create_optimizer(\n",
        "      init_lr=init_lr,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      optimizer_type='adamw')\n",
        "\n",
        "  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
        "  classifier_model.summary()\n",
        "  classifier_model.fit(\n",
        "      x=train_dataset, \n",
        "      validation_data=validation_dataset,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      validation_steps=validation_steps)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fine tuning https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1 model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n",
            "  [n for n in tensors.keys() if n not in ref_input_names])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"prediction\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_mask (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_word_ids (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (KerasLayer)            {'encoder_outputs':  9591041     input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "                                                                 input_word_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 256)          0           encoder[0][3]                    \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 2)            514         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 9,591,555\n",
            "Trainable params: 9,591,554\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "887/887 [==============================] - 569s 636ms/step - loss: 0.3145 - accuracy: 0.8216 - val_loss: 0.0887 - val_accuracy: 0.9695\n",
            "Epoch 2/2\n",
            "887/887 [==============================] - 563s 634ms/step - loss: 0.0128 - accuracy: 0.9962 - val_loss: 0.0894 - val_accuracy: 0.9709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CL4hObtltnD",
        "outputId": "f4991e03-2e41-4522-ea91-21eb58b8a54c"
      },
      "source": [
        "main_save_path = \"/content/drive/My Drive/name_matching_attempt/name_matching_model_v1_r6_with_data_augmentation_text\"\n",
        "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
        "saved_model_name = f'{\"name_matching_v1\"}_{bert_type}'\n",
        "\n",
        "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
        "\n",
        "#preprocess_inputs = bert_preprocess_model.inputs\n",
        "#bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
        "#bert_outputs = classifier_model(bert_encoder_inputs)\n",
        "#model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
        "model_for_export = classifier_model\n",
        "\n",
        "print('Saving', saved_model_path)\n",
        "model_for_export.summary()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving /content/drive/My Drive/name_matching_attempt/name_matching_model_v1_r6_with_data_augmentation_text/name_matching_v1_bert_en_uncased_L-2_H-256_A-4\n",
            "Model: \"prediction\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_mask (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_word_ids (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (KerasLayer)            {'encoder_outputs':  9591041     input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "                                                                 input_word_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 256)          0           encoder[0][3]                    \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 2)            514         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 9,591,555\n",
            "Trainable params: 9,591,554\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhTDT8OT2ltN",
        "outputId": "598b931f-929c-4f7c-dff6-f0729b0bd371"
      },
      "source": [
        "model_for_export.input"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_mask')>,\n",
              " 'input_type_ids': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_type_ids')>,\n",
              " 'input_word_ids': <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_word_ids')>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsgUFoDL2of0",
        "outputId": "d71f755d-882e-4ed4-ba28-c7d4d304ea53"
      },
      "source": [
        "model_for_export.output"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'classifier')>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYTyNRJKzmft",
        "outputId": "b665f53d-d4dd-4d30-92ec-8f79a0d4527e"
      },
      "source": [
        "model_for_export.save(saved_model_path)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 160). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 160). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FpzmVKpm5Sp"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHPan508TOiM",
        "outputId": "e1b0c63e-8f46-4c0c-8b72-8553f09dcca6"
      },
      "source": [
        "!pip install bert-for-tf2 # Just for tokenization, do not use it for model layer\n",
        "from bert import bert_tokenization"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_RVAWFXTY1O"
      },
      "source": [
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/2\", trainable=True)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnqAmbr3uWqD",
        "outputId": "b06250cf-3d90-493d-fb02-bd6c4bda2815"
      },
      "source": [
        "do_lower_case"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTka2OyxT8bd"
      },
      "source": [
        "import numpy\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()\n",
        "\n",
        "def process_data(address_left, address_right, labels, max_seq_len = 256):\n",
        "  assert len(address_left) == len(address_right)\n",
        "  #assert len(address_right) == len(labels)\n",
        "  input_id_list = []\n",
        "  input_mask_list = []\n",
        "  segment_id_list = []\n",
        "  for example_index in range(len(address_left)):\n",
        "      the_address_left_tokens = tokenizer.tokenize(address_left[example_index])\n",
        "      the_address_right_tokens = tokenizer.tokenize(address_right[example_index])\n",
        "      _truncate_seq_pair(the_address_left_tokens, the_address_right_tokens, max_seq_len - 3)\n",
        "      tokens = [\"[CLS]\"] + the_address_left_tokens+ [\"[SEP]\"] + the_address_right_tokens + [\"[SEP]\"]\n",
        "      segment_ids = ([0] * (len(the_address_left_tokens) + 2 )) + ([1] * (len(the_address_right_tokens) + 1 ))\n",
        "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "      input_mask = [1] * len(input_ids)\n",
        "      padding = [0] * (max_seq_len - len(input_ids))\n",
        "      input_ids += padding\n",
        "      input_mask += padding\n",
        "      segment_ids += padding\n",
        "      assert len(input_ids) == max_seq_len\n",
        "      assert len(input_mask) == max_seq_len\n",
        "      assert len(segment_ids) == max_seq_len\n",
        "      input_id_list.append(input_ids)\n",
        "      input_mask_list.append(input_mask)\n",
        "      segment_id_list.append(segment_ids)\n",
        "  return input_id_list, input_mask_list, segment_id_list"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKKEztJwtu_g",
        "outputId": "c354bfc3-e4c2-4111-aba3-35f635dfffc5"
      },
      "source": [
        "# do_lower_case = true\n",
        "import numpy as np\n",
        "t_input_id_list, t_input_mask_list, t_segment_id_list = process_data([\"RELIABLE SHIPPING\"], ['GE CAPITAL EUROPE LIMITED'], None)\n",
        "print(np.array(t_input_id_list), np.array(t_input_mask_list), np.array(t_segment_id_list))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  101 10539  7829   102 16216  3007  2885  3132   102     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]] [[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0]] [[0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ginVhNVmWZ4a",
        "outputId": "eeb6399e-d74f-434e-a384-28666f8c9af4"
      },
      "source": [
        "test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'])\n",
        "test_text = [np.array([\"RELIABLE SHIPPING\"]),\n",
        "             np.array(['GE CAPITAL EUROPE LIMITED'])]\n",
        "text_preprocessed = test_preprocess_model(test_text)\n",
        "\n",
        "print('Keys           : ', list(text_preprocessed.keys()))\n",
        "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
        "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
        "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
        "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
        "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
        "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys           :  ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape Word Ids :  (1, 256)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[  101 10539  7829   102 16216  3007  2885  3132   102     0     0     0\n",
            "     0     0     0     0], shape=(16,), dtype=int32)\n",
            "Shape Mask     :  (1, 256)\n",
            "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
            "Shape Type Ids :  (1, 256)\n",
            "Type Ids       :  tf.Tensor([0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pY3DHFGQMsg",
        "outputId": "669c5324-6418-4090-c745-c2a39b3e3eb4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKHMkF5dQACx"
      },
      "source": [
        "main_save_path = \"/content/drive/My Drive/name_matching_attempt/name_matching_model_v1_r6_with_data_augmentation_text\"\n",
        "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
        "saved_model_name = f'{\"name_matching_v1\"}_{bert_type}'\n",
        "saved_model_path = os.path.join(main_save_path, saved_model_name)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-f-_ZZTm4Ii"
      },
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "246NLtKCyh76",
        "outputId": "20c6f264-f5e2-4ff0-e66b-9b0df2e9e699"
      },
      "source": [
        "reloaded_model.signatures"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, input_word_ids, input_mask, input_type_ids) at 0x7F8001EDC650>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWXzQpgYRsw4",
        "outputId": "8f1c8c99-388a-4f6d-87de-5564f46885bb"
      },
      "source": [
        "type(reloaded_model)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDr3N3NyPdKB",
        "outputId": "07dac882-01e2-464c-e72b-2a655fa31f0c"
      },
      "source": [
        "tinput_id, tinput_mask, tsegment_id = process_data(['RELIABLE SHIPPING'], ['GE CAPITAL EUROPE LIMITED'], None)\n",
        "result = reloaded_model({'input_mask': tf.constant(tinput_mask, dtype=tf.int32), 'input_type_ids': tf.constant(tsegment_id, dtype=tf.int32), 'input_word_ids': tf.constant(tinput_id, dtype=tf.int32)})\n",
        "print(result)\n",
        "print(tf.argmax(result, axis=1)[0])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.9986733  0.00132668]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ILvmDWcqvxo",
        "outputId": "ce083a78-57ad-4578-ba17-c8e47ae940a1"
      },
      "source": [
        "tinput_id, tinput_mask, tsegment_id = process_data(['VITESSE'], ['FOUR (HOLDINGS) LIMITED'], None)\n",
        "result = reloaded_model({'input_mask': tf.constant(tinput_mask, dtype=tf.int32), 'input_type_ids': tf.constant(tsegment_id, dtype=tf.int32), 'input_word_ids': tf.constant(tinput_id, dtype=tf.int32)})\n",
        "print(result)\n",
        "print(tf.argmax(result, axis=1)[0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[9.9977368e-01 2.2633336e-04]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQyalJ_fJiG7",
        "outputId": "86534c50-c775-4534-de48-aadb57b3b9d1"
      },
      "source": [
        "tinput_id, tinput_mask, tsegment_id = process_data(['KGM UNDERWRITING', \"test\", \"THE COOPER GROUP\"], ['TESSUTI LIMITED', \"test2\", \"UTMOST GROUP\"], None)\n",
        "result = reloaded_model({'input_mask': tf.constant(tinput_mask, dtype=tf.int32), 'input_type_ids': tf.constant(tsegment_id, dtype=tf.int32), 'input_word_ids': tf.constant(tinput_id, dtype=tf.int32)})\n",
        "print(result)\n",
        "print(tf.argmax(result, axis=1)[0])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[9.997526e-01 2.474208e-04]\n",
            " [4.181787e-03 9.958182e-01]\n",
            " [9.280080e-01 7.199199e-02]], shape=(3, 2), dtype=float32)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}