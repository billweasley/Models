{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Name Model Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEt-8FQSJg6T",
        "outputId": "ac179efc-227a-480f-9a2e-32b6cb72fc2d"
      },
      "source": [
        "!pip install -q -U tensorflow-text==2.4.1\n",
        "!pip install -q -U tf-models-official==2.4.0\n",
        "!pip install -U tfds-nightly\n",
        "!pip install -q -U tensorflow==2.4.1"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 8.1MB/s \n",
            "\u001b[?25hRequirement already up-to-date: tfds-nightly in /usr/local/lib/python3.7/dist-packages (4.3.0.dev202106210107)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.1.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.4)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (21.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->tfds-nightly) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oP4-y9wKm_J"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSBhACRTK2ay",
        "outputId": "f160ee70-22a6-48fb-b226-ba858fbeadc4"
      },
      "source": [
        "import os\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "  print('Using TPU')\n",
        "elif tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')\n",
        "else:\n",
        "  raise ValueError('Running on CPU is not recommended.')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd2AHbvkLMxM"
      },
      "source": [
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Yrcp5mP6Lgzt",
        "outputId": "85e3fc5c-990a-433d-e718-665b0720c790"
      },
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-256_A-4'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_uncased_L-24_H-1024_A-16\", \"bert_en_wwm_uncased_L-24_H-1024_A-16\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_en_cased_L-24_H-1024_A-16\", \"bert_en_wwm_cased_L-24_H-1024_A-16\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"albert_en_large\", \"albert_en_xlarge\", \"albert_en_xxlarge\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\", \"talking-heads_large\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_large/2',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xlarge/2',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xxlarge/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print('BERT model selected           :', tfhub_handle_encoder)\n",
        "print('Preprocessing model auto-selected:', tfhub_handle_preprocess)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1\n",
            "Preprocessing model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVjfKHqFMU-i"
      },
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=256):\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: a list with the names of string-valued features.\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\n",
        "  # Simple cases (like this example) can skip this step and let\n",
        "  # the next step apply a default truncation to approximately equal lengths.\n",
        "  truncated_segments = segments\n",
        "\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL11ug6rQftc"
      },
      "source": [
        "def build_classifier_model(num_classes = 2):\n",
        "  inputs = dict(\n",
        "      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "  )\n",
        "\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\n",
        "  net = encoder(inputs)['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(rate=0.1)(net)\n",
        "  net = tf.keras.layers.Dense(num_classes, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(inputs, net, name='prediction')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDRNkb_lXXoH",
        "outputId": "5316d650-9749-41f6-ee0e-22bc11931c79"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K_IJJEfXLJR"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/name_matching_attempt/combined_negative_samples.pickle\", \"rb\") as handle:\n",
        "  combined_negative = pickle.load(handle)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67lxLREzXmhV"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/name_matching_attempt/combined_positive_samples.pickle\", \"rb\") as handle:\n",
        "  combined_positive = pickle.load(handle)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO5Hu3XBXuJy"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 10000)\n",
        "pd.set_option('max_colwidth', 10000)\n",
        "pd.set_option(\"max_rows\", 50000)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiqmQVFQX1Lo"
      },
      "source": [
        "list_positive_left = combined_positive['name_left'].tolist()\n",
        "list_positive_right = combined_positive['name_right'].tolist()\n",
        "list_negative_left = combined_negative['name_left'].tolist()\n",
        "list_negative_right = combined_negative['name_right'].tolist()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aZrFug3YmLZ"
      },
      "source": [
        "list_positive_pairs = [[left, right] for left, right in zip(list_positive_left, list_positive_right)]\n",
        "list_negative_pairs = [[left, right] for left, right in zip(list_negative_left, list_negative_right)]\n",
        "\n",
        "n_training_positive = int(len(list_positive_pairs) * 0.8)\n",
        "n_training_negative = int(len(list_negative_pairs) * 0.8)\n",
        "\n",
        "labels_training = [1 for _ in range(n_training_positive)] + [0 for _ in range(n_training_negative)]\n",
        "samples_training = list_positive_pairs[: n_training_positive] + list_negative_pairs[:n_training_negative]\n",
        "\n",
        "labels_validation = [1 for _ in range(len(list_positive_pairs[n_training_positive + 1:]))] + [0 for _ in range(len(list_negative_pairs[n_training_negative + 1:]))]\n",
        "samples_validation = list_positive_pairs[n_training_positive + 1:] + list_negative_pairs[n_training_negative+1:]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a6ql_1rZCjX",
        "outputId": "15bc31a0-a51e-421c-daf6-792eea0c3ff1"
      },
      "source": [
        "len(samples_training)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32224"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_iW-xTLbhdZ",
        "outputId": "2b354ef7-867d-4772-e50d-5d5523ff70e8"
      },
      "source": [
        "len(samples_validation)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8054"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLm8ocFHbqEw"
      },
      "source": [
        "ds_training_dict = {\n",
        "    'left_name': [sample[0] for sample in samples_training], \n",
        "    'right_name': [sample[1] for sample in samples_training],\n",
        "    'label': labels_training,\n",
        "    }\n",
        "ds_validation_dict = {\n",
        "    'left_name': [sample[0] for sample in samples_validation], \n",
        "    'right_name': [sample[1] for sample in samples_validation],\n",
        "    'label': labels_validation,\n",
        "    }"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QYcXsx7TcEY"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_dataset(ds, is_training, batch_size, bert_preprocess_model):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
        "  num_examples = len(ds['left_name'])\n",
        "  \n",
        "  dataset = dataset.shuffle(num_examples)\n",
        "  if is_training:\n",
        "    dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
        "  dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  return dataset, num_examples"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATOrHhyRmyO"
      },
      "source": [
        "def get_configuration():\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        'val_accuracy', dtype=tf.float32)\n",
        "\n",
        "  return metrics, loss"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cbq1vgHSTYB",
        "outputId": "6b8fd99b-30a5-4f49-b60e-5ee29f11fb71"
      },
      "source": [
        "epochs = 2\n",
        "batch_size = 64\n",
        "init_lr = 1e-5\n",
        "\n",
        "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
        "bert_preprocess_model = make_bert_preprocess_model(['left_name', 'right_name'])\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "  # metric have to be created inside the strategy scope\n",
        "  metrics, loss = get_configuration()\n",
        "\n",
        "  train_dataset, train_data_size = load_dataset(ds_training_dict, True, batch_size, bert_preprocess_model)\n",
        "  steps_per_epoch = train_data_size // batch_size\n",
        "  num_train_steps = steps_per_epoch * epochs\n",
        "  num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "  validation_dataset, validation_data_size = load_dataset(ds_validation_dict, False, batch_size, bert_preprocess_model)\n",
        "  validation_steps = validation_data_size // batch_size\n",
        "\n",
        "  classifier_model = build_classifier_model()\n",
        "\n",
        "  optimizer = optimization.create_optimizer(\n",
        "      init_lr=init_lr,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      optimizer_type='adamw')\n",
        "\n",
        "  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
        "  classifier_model.summary()\n",
        "  classifier_model.fit(\n",
        "      x=train_dataset,\n",
        "      validation_data=validation_dataset,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      validation_steps=validation_steps)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fine tuning https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1 model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n",
            "  [n for n in tensors.keys() if n not in ref_input_names])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"prediction\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (KerasLayer)            {'encoder_outputs':  9591041     input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 256)          0           encoder[0][3]                    \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 2)            514         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 9,591,555\n",
            "Trainable params: 9,591,554\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "503/503 [==============================] - 52s 96ms/step - loss: 0.2086 - val_accuracy: 0.9233 - val_loss: 0.0238 - val_val_accuracy: 0.9937\n",
            "Epoch 2/2\n",
            "503/503 [==============================] - 47s 94ms/step - loss: 0.1022 - val_accuracy: 0.9697 - val_loss: 0.0216 - val_val_accuracy: 0.9940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CL4hObtltnD",
        "outputId": "4261c9a1-6f4a-44ce-930e-1b6c1d2857e8"
      },
      "source": [
        "main_save_path = \"/content/drive/My Drive/name_matching_attempt/name_matching_model_v1_r4\"\n",
        "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
        "saved_model_name = f'{\"name_matching_v1\"}_{bert_type}'\n",
        "\n",
        "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
        "\n",
        "preprocess_inputs = bert_preprocess_model.inputs\n",
        "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
        "bert_outputs = classifier_model(bert_encoder_inputs)\n",
        "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
        "\n",
        "print('Saving', saved_model_path)\n",
        "model_for_export.summary()\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving /content/drive/My Drive/name_matching_attempt/name_matching_model_v1_r4/name_matching_v1_bert_en_uncased_L-2_H-256_A-4\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "left_name (InputLayer)          [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "right_name (InputLayer)         [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_2 (Functional)            {'input_mask': (None 0           left_name[0][0]                  \n",
            "                                                                 right_name[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "prediction (Functional)         (None, 2)            9591555     model_2[0][0]                    \n",
            "                                                                 model_2[0][1]                    \n",
            "                                                                 model_2[0][2]                    \n",
            "==================================================================================================\n",
            "Total params: 9,591,555\n",
            "Trainable params: 9,591,554\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhTDT8OT2ltN",
        "outputId": "ca30bb73-bf15-421e-e8c7-7442d10130ef"
      },
      "source": [
        "model_for_export.input"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(None,) dtype=string (created by layer 'left_name')>,\n",
              " <KerasTensor: shape=(None,) dtype=string (created by layer 'right_name')>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsgUFoDL2of0",
        "outputId": "1685587d-e9b3-4754-b8a3-17ea0f5fe531"
      },
      "source": [
        "model_for_export.output"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'prediction')>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYTyNRJKzmft",
        "outputId": "5d573204-d412-42b4-ded7-2dc1f1e75f03"
      },
      "source": [
        "model_for_export.save(saved_model_path)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 160). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FpzmVKpm5Sp"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7MAOlpToK4P"
      },
      "source": [
        "ds_validation_dict_for_test = {\n",
        "    'left_name': [sample[0] for sample in samples_validation], \n",
        "    'right_name': [sample[1] for sample in samples_validation],\n",
        "}"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKHMkF5dQACx"
      },
      "source": [
        "main_save_path = \"/content/drive/My Drive/name_matching_attempt/name_matching_model_v1_r3\"\n",
        "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
        "saved_model_name = f'{\"name_matching_v1\"}_{bert_type}'\n",
        "saved_model_path = os.path.join(main_save_path, saved_model_name)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pY3DHFGQMsg",
        "outputId": "ad5ee17f-dadd-49af-c8f0-2f4597bf5218"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-f-_ZZTm4Ii"
      },
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "246NLtKCyh76",
        "outputId": "bad00bd6-e931-4738-bdd9-b7e284e4d376"
      },
      "source": [
        "reloaded_model.signatures"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_SignatureMap({'serving_default': <ConcreteFunction signature_wrapper(*, left_name, right_name) at 0x7F6F092BF950>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWXzQpgYRsw4",
        "outputId": "6e032a72-7968-4687-bc24-aa4d30f621eb"
      },
      "source": [
        "type(reloaded_model)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1OOIAwlm_bN",
        "outputId": "1067c278-5d4e-4917-8f44-a17819496a8f"
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices(ds_validation_dict_for_test)\n",
        "for test_row in test_dataset.shuffle(1000).take(5):\n",
        "  model_inputs = [[test_row[ft]] for ft in test_row]\n",
        "  result = reloaded_model(model_inputs)\n",
        "  print(model_inputs, tf.argmax(result, axis=1)[0])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[<tf.Tensor: shape=(), dtype=string, numpy=b'SUNCOURT '>], [<tf.Tensor: shape=(), dtype=string, numpy=b'SUNCOURT HEALTHCARE '>]] tf.Tensor(1, shape=(), dtype=int64)\n",
            "[[<tf.Tensor: shape=(), dtype=string, numpy=b'TIAKYFEE CONSULTANTS '>], [<tf.Tensor: shape=(), dtype=string, numpy=b'TIAKYFFE CONSULTANTS LIMITED'>]] tf.Tensor(1, shape=(), dtype=int64)\n",
            "[[<tf.Tensor: shape=(), dtype=string, numpy=b'MADAME BUTTERFLY\\xe2\\x80\\x99S VINTAGE BOUTIQUE '>], [<tf.Tensor: shape=(), dtype=string, numpy=b'MADAM BUTTERFLY VINTAGE BOUTIQUE '>]] tf.Tensor(1, shape=(), dtype=int64)\n",
            "[[<tf.Tensor: shape=(), dtype=string, numpy=b'RICHARD SMYTHE '>], [<tf.Tensor: shape=(), dtype=string, numpy=b'RICHARD SMYTHE FIREPLACES '>]] tf.Tensor(1, shape=(), dtype=int64)\n",
            "[[<tf.Tensor: shape=(), dtype=string, numpy=b'BENTLEYS GARDEN BUILDINGS '>], [<tf.Tensor: shape=(), dtype=string, numpy=b'BENTLEYS ROSES '>]] tf.Tensor(1, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ILvmDWcqvxo",
        "outputId": "f49dd094-55b0-4f8d-f2ed-5166205facbf"
      },
      "source": [
        "print(reloaded_model([[tf.constant('YOUR DIGITAL ACCOUNTANT LIMITED')], [tf.constant('Hello World')]]))\n",
        "print(tf.argmax(reloaded_model([[tf.constant('YOUR DIGITAL ACCOUNTANT LIMITED')], [tf.constant('Hello World')]]), axis=1)[0])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[ 1.8928653 -3.307738 ]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}